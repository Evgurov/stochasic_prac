\anonsection{Задание 3}
\anonsubsection{Формулировка задания}
\begin{enumerate}
	\item Построить датчик экспоненциального распределения. Проверить для
     данного распределения свойство отсутствия памяти. Пусть $X_1, X_2,
     \dots, X_n$ --- независимо экспоненциально распределенные с. в. с
     параметрами $\lambda_1, \lambda_2, \dots, \lambda_n$ соответственно.
     Найти распределение случайной величины $Y = \min(X_1, X_2, \dots, X_n)$.
	\item На основе датчика экспоненциального распределения построить датчик
     пуассоновского распределения.
	\item Построить датчик пуассоновского распределения как предел
     биномиального распределения. С помощью критерия хи-квадрат Пирсона
     убедиться, что получен датчик распределения Пуассона.
	\item Построить датчик стандартного нормального распределения методом
     моделирования случайных величин парами с переходом в полярные координаты.
     Проверить при помощи t-критерия Стьюдента равенство математических
     ожиданий, а при помощи критерия Фишера равенство дисперсий.  
\end{enumerate}

\anonsubsection{Датчик экспоненциального распределения}
\begin{definition}
	Случайная величина $X$ имеет экспоненциальное распределение с параметром
     $\lambda > 0$, если ее функция распределения имеет вид:
	\begin{equation}\label{exp_func}
	    F_X(x) = 
        \begin{cases}
	        1 - e^{-\lambda x}, &x \geqslant 0,\\
	        0, &x < 0.
	    \end{cases}
	\end{equation}
\end{definition}

\begin{theorem}[Метод обратной функции]
    Пусть функция распредления $ F $ имеет обратную $ F^{-1} $. Тогда
     функцией распределения случайной величины
    $$
     X = F^{-1}(Y),
    $$
     где $ Y \sim \Uni[0,1]$, является $ F $.
\end{theorem}
\begin{proof}
    Найдем функцию распределения $ X $:
    $$
     F_X(x) = \mathbb{P}(X < x) = \mathbb{P}(F^{-1}(Y) < x) =
      \mathbb{P}(Y < F(x)) = F(x).
    $$
\end{proof}

В случае экспоненциального распределения функция распределения \eqref{exp_func}
 удовлетворяет условиям теоремы и обратная к ней легко выражается:
$$
    F_X^{-1}(y) = -\frac{1}{\lambda} \ln(1 - y).
$$
Суперпозиция $ F(Y) $,где $ Y \sim \Uni[0,1] $ является случайной величиной,
 имеющей экспоненциальное распределение с параметром $ \lambda $:
$$
 X = -\frac{1}{\lambda} \ln(1 - Y) \sim \Expon(\lambda).
$$
На Рис.\eqref{fig:exp_dist} приведено сравнение полученной эмпирически,
 с помощью построенного датчика, плотности экспоненциального распределения
 и его теоретической плотности, представимой в виде:
$$
p(x) = \lambda e^{-\lambda x}
$$
при $ \lambda = 0.5 $.

 \begin{figure}[ht]
	\centering
	\includegraphics[width = 0.8\linewidth]{"./resources/exp_dist.png"}
	\caption{Эмпирическая и теоретическая плотности экспоненциального
     распределения при $ \lambda = 0.5 $.}
    \label{fig:exp_dist}
\end{figure}

\anonsubsection{Свойство отсутствия памяти}
Экспоненциальное распределение, как и его дискретный аналог --- геометрическое,
 обладает свойством отсутствия мапяти, которое в данном случае можно сформулировать
 как
\begin{statement}
	Случайная величина $X\sim \Expon(\lambda)$ обладает свойством отсутствия памяти,
     то есть $ \forall s,t \ge 0 $ следует, что
	\begin{equation}\label{expmem}
	    \mathbb{P} (X \ge s + t \mid X \ge t) = \mathbb{P} (X \ge s). 
	\end{equation}
\end{statement}
\begin{proof}
	$$
	\mathbb{P}(X \ge s + t \mid X \ge t) = \frac{\mathbb{P}(X \ge s + t, X \ge t}
	 {\mathbb{P}(X \ge t)} = \frac{\mathbb{P} (X \ge s + t)}{\mathbb{P}(t \ge t)}
	 = \mathbb{P}(X \ge s). 
	$$
	Таким образом, получаем:
	\begin{equation}
		\mathbb{P}(X\ge s+t)=\mathbb{P}(X\ge t)\mathbb{P}(X\ge s). \label{equi}
	\end{equation}
	Для экспоненциально распределенной случайной величины верно, что:
	$$
	\mathbb{P} (X \ge t) = 1 - F_X(t) = e^{-\lambda t}, \quad
	\mathbb{P} (X \ge s + t) = e^{-\lambda(s + t)}.
	$$
	Следовательно, для \eqref{equi} выполняется:
	$$
	e^{-\lambda(s + t)} = e^{-\lambda s} e^{-\lambda t}.
	$$
	Следовательно, экспоненциальное распределение обладает свойством отсутствия
	 памяти.
\end{proof}
На Рис.\eqref{fig:exp_memoryless}, аналогично геометрическому распределению,
 данное свойство проиллюстрировано эмпирически.
\begin{figure}[ht]
	\centering
	\includegraphics[width = 0.8\linewidth]{"./resources/exp_memoryless.png"}
	\caption{Эмпирическая иллюстрация свойства отсутствия памяти при t = 2.}
    \label{fig:exp_memoryless}
\end{figure}

\anonsubsection{Случайная величина $ Y = \min(X_1, X_2, \dots, X_n) $}
\begin{statement}
	Пусть $ X_1,X_2,\ldots,X_n $ --- независимые экспоненциально распределённые
	 случайные величины с параметрами $ \lambda_1,\lambda_2,\ldots,\lambda_n $
	 соответственно. Тогда случайная величина $ Y = \min(X_1, X_2, \dots, X_n)
	 \sim \Expon \left( \sum\limits_{i = 1}^n \lambda_i \right) $.
\end{statement}
\begin{proof}
	\begin{multline*}
		F_Y(x) = \mathbb{P}(Y \le x) = 1 - \mathbb{P} (Y > x) = 1 - \mathbb{P}
		 (\min(X_1, X_2, \dots, X_n) > x) = \\
		= 1 - \mathbb{P}(X_1 > x, X_2 > x, \dots, X_n > x) = \left\{ X_1, X_2,
		 \dots, X_n \text{ независимы} \right\} = \\
		= 1 - \mathbb{P} (X_1 > x) \cdot \mathbb{P} (X_2 > x) \cdot \dots
		 \mathbb{P} (X_n > x) = \\
		= 1 - \left( 1 - F_{X_1}(x) \right) \cdot \left( 1 - F_{X_2}(x) \right) 
		\cdot \dots \cdot \left( 1 - F_{X_n}(x) \right) = \\
		= 1 - e^{-\lambda_1 x} \cdot e^{-\lambda_2 x} \cdot \dots \cdot
		 e^{-\lambda_n x} = 1 - e^{-\left( \sum_{i=1}^n \lambda_i \right) x}.
	\end{multline*}
\end{proof}
Эмпирическая демонстрация этого факта для n = 4, и случайно сгенерированных
 в интервале от 0 до 0.1 параметров $\lambda_i $ приведена на Рис.\eqref{fig:exp_min}.
\begin{figure}[ht]
	\centering
	\includegraphics[width = 0.8\linewidth]{"./resources/exp_min.png"}
	\caption{Расределение $ Y = \min(X_1, \dots, X_n) $.}
    \label{fig:exp_min}
\end{figure}

\anonsubsection{Датчик пуассоновского распределения}
\begin{definition}
	Случайная величина $X$ имеет распределение Пуассона с параметром $\lambda>0$,
	 если
	$$
	\mathbb{P}(X = k) = \frac{\lambda^k}{k!} e^{-\lambda}, \quad k \in \mathbb{N} 
	\cup \{0\}.
	$$
\end{definition}
Удобный метод построения датчика пуассоновского распределения даёт следующая 
\begin{theorem}
	Пусть $X_1,X_2,\ldots,X_n,\ldots\sim Exp(\lambda)$ --- независимые одинаково
	 распредленные случайные величины. Тогда случайная величина, определенная
	 следующим образом:
	$$
	Y = \max(n \mid S_n = X_1 + X_2 + \dots + X_n < 1)
	$$
	имеет распределение Пуассона с параметром $\lambda$. При этом полагается $Y=0$,
	 если таких $ n $ не существует.
\end{theorem}
Доказательство теоремы можно найти в \cite{model_randoms} на стр. 34. Таким образом
 для моделирования случайной величины Пуассона можно последовательно генерировать
 показательные случайные величины, пока их сумма не станет больше единицы. Количество
 сгенерированных экспоненциальных величин минус один и будет значением пуассоновской
 случайной величины. На Рис.\eqref{fig:pois_dist} изображено сравнение распределения
 выборки полученной с помощью построенного вышеописанным способом датчика и
 теоретической функции вероятности.

\begin{figure}[ht]
	\centering
	\includegraphics[width = 0.8\linewidth]{"./resources/pois_dist.png"}
	\caption{Эмпирическая и теоретическая плотности распределения Пуассона при
	 $ \lambda = 4 $.}
    \label{fig:pois_dist}
\end{figure}

\anonsubsection{Датчик пуассоновского распределения как как предел биномиального
 распределения}
Другой способ моделирования пуассоновской случайной величины основывается на 
 следующей предельной теореме, связывающей распределение Пуассона с биномиальным
 распределением.
Пусть
$$
P_n(k) = 
\begin{cases}
	C_n^k p^k q^{n-k}, & k = 0, 1, \dots, n,\\
	0, & k = n + 1, n + 2, \dots, 
\end{cases}
$$
и пусть $ p $ является функцией от $ n, \ p = p(n) $.
\begin{theorem}[Пуассона]
	Пусть $ p(n) \to 0, n \to \infty $, причем так, что $ n p(n) \to \lambda $,
	 где $ \lambda > 0 $. Тогда для любого $ k = 0, 1, \dots $
	$$
	P_n(k) \to \dfrac{\lambda^k e^{-k}}{k!}, \quad k = 0, 1, \dots .
	$$
\end{theorem}
Доказательство этой теоремы можно найти в \cite{shir_prob} на стр. 90. Таким
 образом строить датчик распределения Пуассона с параметром $ \lambda $
 можно с помощью датчика биномиального распределения при $ p =
 \dfrac{\lambda}{n} $ и больших значениях $ n $. На Рис.\eqref{fig:pois_binlim}
 проиллюстрировано достаточно хорошее совпадение распределений $ \Bin\left( n,
 \dfrac{\lambda}{n} \right) $ и $ \Pois(\lambda) $ при $ n = 10000, \lambda = 10 $.

\begin{figure}[ht]
	\centering
	\includegraphics[width = 0.8\linewidth]{"./resources/pois_binlim.png"}
	\caption{Демонстрация предельного совпадения биномиального и пуассоновского
	 распределений при $ n = 10000, \lambda = 10 $.}
    \label{fig:pois_binlim}
\end{figure}

\anonsubsection{Проверка корректности датчика и критерий хи-квадрат Пирсона}
Проверим корректность построенного с помощью биномиального распределения датчика.
 Для этого воспользуемся криетрием хи-квадрат Пирсона, но для начала дадим
 необходимые определения.
\begin{definition}
	Пусть случайные величины $ Z_1, \dots, Z_k $ распределены по стандартному
	 нормальному закону $ \mathcal{N}(0,1) $ и независимы. Тогда распределние
	 случайной величины $ R_k^2 = Z_1^2 + \dots + Z_k^2 $ называют распределением
	 хи-квадрат с $ k $ степенями свободы(кратко: $ R_k^2 \sim \chi_k^2) $. 
\end{definition}
Пусть $ X_1, \dots, X_n $ --- выборка из закона с функцией распределения $ F(x) $.
 Разобьем множество значений $ X_1 $ на $ N $ промежутков(возможно бесконечных)
 $ \delta_j = (a_j, b_j], \quad j = 1, \dots, N$. В случае дискретных распределений
 вместо промежутков значений можно рассматривать отдельные значения. 
 Положим $ p_j = \mathbb{P}(X_1 \in \delta_j) $, а случайные величины $ \nu_j $ ---
 равными количеству элементов выборки в $ \delta_j \ (\nu_1 + \dots + \nu_N = n) $.
 Функция $ F $ неизвестна и проверяется гипотеза
$$
 H_0: F(x) = F_0(x),
$$
 где $ F_0 $ --- заданная функция распределения. Если гипотеза верна, то согласно
 закону больших чисел частоты попадания в промежутки $ \hat{p}_j = \dfrac{\nu_j}{n} $
 при достаточно больших $ n $ должны быть близки к соответствуюшим вероятностям
 $ p_j^0 = F_0(b_j) - F_0(a_j) $. В качетсве меры отклонения от гипотезы $ H_0 $
 принимается статистика
$$
X_n^2 = n \sum_{j = 1}^N \frac{1}{p_j^0}(\hat{p}_j - p_j^0)^2 = \sum_{j = 1}^N
 \frac{(\nu_j - np_j^0)^2}{np_j^0},
$$
которая по сути является взвешенной суммой квадратов отклонений частот от
 гипотетических вероятностей. В силу центральной предельной теоремы
 каждое отклонение асимптотически нормально и имеет порядок малости
 $ \dfrac{1}{\sqrt{n}} $, поэтому представляется правдоподобной следующая
\begin{theorem}
	Если $ 0 < p_j^0 < 1, \quad j = 1, \dots, N,$ то при $ n \to \infty $
	$$
	X_n^2 \xrightarrow{d} \zeta \sim \chi_{N-1}^2.
	$$
\end{theorem}
Здесь сходимость понимается в смысле сходимости по распределению. Доказательство
 этой теоремы можно найти в \cite{lagutin_stat} на стр. 274. Аналогично теореме
 Колмогорова, данная теорема позволяет оценивать вероятность отклонения, задаваемого
 статистикой Пирсона, посчитанного для конкретной выборки и, в зависимости от
 необходимого уровня значимости, принимать или отвергать гипотезу $ H_0 $.